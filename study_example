import glob
import h5py
import numpy as np
import os
from matplotlib import pyplot as plt
from FastLZeroSpikeInference import fast
from scipy.spatial.distance import squareform
from elephant.spike_train_dissimilarity import van_rossum_distance,victor_purpura_distance
import quantities as pq
from neo import SpikeTrain


def read_h5_file(file_path):
    try:
        # Read h5 file
        h5 = h5py.File(file_path, 'r')
        
        # Get data from h5 datastructure
        dto = 1.0 * np.array(h5['dto'])
        dte = 1.0 * np.array(h5['dte'])
        quiroga = 1.0 * np.array(h5['quiroga'])
        dff = np.array(h5['dff']).ravel()
        sptimes = np.array(h5['sptimes']).ravel()
        genotype = h5['genotype'][()].decode("utf-8")
        ephys = np.array(h5['ephys_baseline_subtracted']).ravel()

        return {
            "genotype": genotype,
            "dff": dff,
            "ephys": ephys,
            "dto": dto,
            "dte": dte,
            "sptimes": sptimes,
            "quiroga": quiroga
        }
    
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return None

def plot_data(data_dict, file_name):
    dto, dte, dff, ephys, sptimes, quiroga = (
        data_dict["dto"],
        data_dict["dte"],
        data_dict["dff"],
        data_dict["ephys"],
        data_dict["sptimes"],
        data_dict["quiroga"],
    )

    # Parameters for estimating spikes
    gam = 0.91
    lam = 0.01

    # Estimate spikes using FastLZeroSpikeInference
    fit = fast.estimate_spikes(data_dict["dff"], gam, lam, True, True)
    fit = fast.estimate_calcium(fit)

    # Ensure estimated data has the same length as original data
    estimated_calcium = fit['estimated_calcium'][:len(dff)]


    # Plot original data and estimated calcium values in subplots
    fig, axs = plt.subplots(3, 3, figsize=(18, 15))
    fig.suptitle(file_name)
    

    axs[0, 0].plot(np.arange(0, len(dff) * dto, dto), dff, 'g')
    axs[0, 0].set_ylabel('DF/F')
    axs[0, 0].set_title('Original Data')

    axs[1, 0].plot(np.arange(0, len(ephys) * dte, dte), 1000 * ephys, 'k')
    axs[1, 0].set_ylabel('mV' )
    axs[1, 0].axhline(y=1000.0 * quiroga, color='r', alpha=0.5)
    axs[1, 0].set_title('Ephys')

    axs[2, 0].eventplot(
        sptimes,
        orientation='horizontal',
        lineoffsets=-1,
        linelengths=0.5,
        linewidths=None,
        colors='r',
        linestyles='solid'
    )
    axs[2, 0].set_ylabel('spikes')
    axs[2, 0].set_xlabel('Time [s]')
    axs[2, 0].tick_params(labelleft=False)
    axs[2, 0].set_title('Spike Times')

    axs[0, 2].plot(np.arange(0, len(estimated_calcium) * dto, dto), estimated_calcium, 'b')
    axs[0, 2].set_ylabel('DF/F')
    axs[0, 2].set_title('Estimated Calcium')

    # Plot estimated spike times with the same x-axis range as original spike times
    axs[1, 2].eventplot(
        fit['spikes'] * data_dict["dto"],  # Use data_dict["dto"] 
        orientation='horizontal',
        lineoffsets=-1,
        linelengths=0.5,
        linewidths=None,
        colors='b',
        linestyles='solid'
    )
    axs[1, 2].set_ylabel('spikes')
    axs[1, 2].set_xlabel('Time [s]')
    axs[1, 2].tick_params(labelleft=False)
    axs[1, 2].set_title('Estimated Spike Times')

    plt.savefig(f'{file_name}_traces.png')
    plt.show()



data_directory = "/Users/petrusgynther/Desktop/ca-signal-deconvolution-main"
# Get all .h5 files' names
path = os.path.join(data_directory, "src", "*.h5")
best_score = float('inf')
best_distance = float('inf')

gam = 0.91
lam = 0.01


def count_hits_misses_false_detections(true_spikes, estimated_spikes, temporal_precision=0.50, start_time=0, end_time=None):
    if end_time is None:
        end_time = len(data_dict["dff"]) * data_dict["dto"]  # Use the maximum time value as t_stop

    hits = 0
    misses = 0
    false_detections = 0

    # Create a list to store unmatched estimated spikes
    unmatched_estimated_spikes = list(estimated_spikes)  # Copy estimated_spikes

    # Iterate through each true spike
    for true_spike in true_spikes:
        matched = False

        # Check each estimated spike for a matching true spike
        for estimated_spike in unmatched_estimated_spikes:
            if abs(estimated_spike - true_spike) <= temporal_precision:
                hits += 1
                unmatched_estimated_spikes.remove(estimated_spike)  # Remove from unmatched
                matched = True
                break
        
        # If no match is found, it's a miss
        if not matched:
            misses += 1

    # Calculate false detections as the difference between estimated spikes and hits
    false_detections = len(estimated_spikes) - hits

    return hits, misses, false_detections





def calculate_error_rate(hits, misses, false_detections):
    total_spikes = hits + misses
    total_detections = hits + false_detections
    if total_spikes == 0:
        sensitivity = 0
    else:
        sensitivity = 1 - (misses / total_spikes)
    if total_detections == 0:
        precision = 0
    else:
        precision = 1 - (false_detections / total_detections)
    if sensitivity + precision == 0:
        error_rate = 1
    else:
        error_rate = 1 - (2 * sensitivity * precision) / (sensitivity + precision)
    return error_rate


# Loop through each file
for file_path in glob.glob(path):
    file_name = os.path.split(file_path)[-1].split(".")[0]
    print(f"Processing file: {file_name}")

    # Read data from h5 file
    data_dict = read_h5_file(file_path)
    if data_dict is not None:
        fit = fast.estimate_spikes(data_dict["dff"], gam, lam, True, True)
        fit = fast.estimate_calcium(fit)
        start_time = 0  # Modify this to match your plot's start time
        end_time = len(data_dict["dff"]) * data_dict["dto"] # Modify this to match your plot's end time

        # Filter spikes within the specified time range
        true_spikes_in_range = [spike for spike in data_dict["sptimes"] if start_time <= spike <= end_time]
        estimated_spikes_in_range = [spike * data_dict["dto"] for spike in fit['spikes'] if start_time <= spike * data_dict["dto"] <= end_time]
        print("true spikes in range", true_spikes_in_range)
        print("estimated_spikes_inrange", estimated_spikes_in_range)

        num_spikes_original = len(true_spikes_in_range)
        num_estimated_spikes = len(estimated_spikes_in_range)

        # Calculate hits, misses, and false detections
        hits, misses, false_detections = count_hits_misses_false_detections(
            data_dict['sptimes'], fit['spikes']*data_dict['dto'], temporal_precision=0.5
        )
        print("Number of spikes in the original data:", num_spikes_original)
        print("Number of estimated spikes:", num_estimated_spikes)
        print("Hits:", hits)
        print("Misses:", misses)
        print("False detections:", false_detections)

        # Calculate Error Rate
        error_rate = calculate_error_rate(hits, misses, false_detections)
        print("Error Rate:", error_rate)

        # Calculate Victor-Purpura distance
        q = 1 / (10 * pq.s)
        estimated_spiketrain = SpikeTrain(fit['spikes'] * data_dict["dto"], units='s', t_stop=end_time)
        true_spiketrain = SpikeTrain(data_dict["sptimes"], units='s', t_stop=end_time)
        vp_distance = victor_purpura_distance([true_spiketrain, estimated_spiketrain], q)[0, 1]
        print("Victor-Purpura Distance:", vp_distance)

    

        # Calculate the second Van Rossum distance
        estimated_spiketrain = SpikeTrain(fit['spikes'] * data_dict["dto"], units='s', t_stop=end_time)
        true_spiketrain = SpikeTrain(data_dict["sptimes"], units='s', t_stop=end_time)
        tau_1 = 1 * pq.s
        distance_vr2 = van_rossum_distance([true_spiketrain, estimated_spiketrain], time_constant=tau_1)[0, 1]
        print("Van Rossum distance:", distance_vr2)


        plot_data(data_dict, file_name)   


        
# Set the directory path
data_directory = "/Users/petrusgynther/Desktop/ca-signal-deconvolution-main"
# Get all .h5 files' names
path = os.path.join(data_directory, "src", "*.h5")
gam_values = np.arange(0.01, 1.0, 0.03)
lambda_values = np.arange(0.01, 1.0, 0.03)



best_distance_vp = float('inf')
second_best_distance_vp = float('inf')
best_distance_vr = float('inf')
second_best_distance_vr = float('inf')
best_similarity_schreiber = 0.0  # Initialize with the lowest Schreiber similarity
second_best_similarity_schreiber = 0.0
best_distance_vr2 = float('inf')
second_best_distance_vr2 = float('inf')

best_params_vp = {}
second_best_params_vp = {}
best_params_vr = {}
best_params_vr2 = {}
second_best_params_vr = {}
second_best_params_vr2 = {}


vp_distances_list = []

for gam in gam_values:
    for lam in lambda_values:
        total_distance_vp = 0
        total_distance_vr2 = 0
        
        for file_path in glob.glob(path):
            file_name = os.path.split(file_path)[-1].split(".")[0]
            data_dict = read_h5_file(file_path)
            end_time = len(data_dict["dff"]) * data_dict["dto"]

            if data_dict is not None:
                fit = fast.estimate_spikes(data_dict["dff"], gam, lam, True, True)
                fit = fast.estimate_calcium(fit)
                
                # Calculate Victor-Purpura distance
                q = 1 / (10 * pq.s)
                estimated_spiketrain = SpikeTrain(fit['spikes']*data_dict["dto"], units='s', t_stop=end_time)
                true_spiketrain = SpikeTrain(data_dict['sptimes'], units='s', t_stop=end_time)
                vp_distance = victor_purpura_distance([true_spiketrain, estimated_spiketrain], q)[0, 1]
                total_distance_vp += vp_distance

                # Calculate 2nd Van Rossum distance
                estimated_spiketrain = SpikeTrain(fit['spikes']*data_dict['dto'], units='s', t_stop=end_time)
                true_spiketrain = SpikeTrain(data_dict["sptimes"], units='s', t_stop=end_time)
                tau_1 = 1 * pq.s
                distance_vr2 = van_rossum_distance([true_spiketrain, estimated_spiketrain], time_constant=tau_1)[0, 1]
                total_distance_vr2 += distance_vr2

                # Update best and second best parameters for Victor-Purpura distance
                if total_distance_vp < best_distance_vp:
                    second_best_distance_vp = best_distance_vp
                    second_best_params_vp = best_params_vp
                    best_distance_vp = total_distance_vp
                    best_params_vp = {'gamma': gam, 'lambda': lam}
                elif total_distance_vp < second_best_distance_vp:
                    second_best_distance_vp = total_distance_vp
                    second_best_params_vp = {'gamma': gam, 'lambda': lam}
                
                # Update best and second best parameters for Van Rossum distance
                if distance_vr2 < best_distance_vr2:
                    second_best_distance_vr2 = best_distance_vr2
                    second_best_params_vr2 = best_params_vr2
                    best_distance_vr2 = distance_vr2
                    best_params_vr2 = {'gamma': gam, 'lambda': lam}
                elif distance_vr2 < second_best_distance_vr2:
                    second_best_distance_vr2 = distance_vr2
                    second_best_params_vr2 = {'gamma': gam, 'lambda': lam}   

        vp_distances_list.append({'gamma': gam, 'lambda': lam, 'total_distance': total_distance_vp})

# Print the results for Victor-Purpura distance
print("Best Parameters for Victor-Purpura Distance:", best_params_vp)
print("Second Best Parameters for Victor-Purpura Distance:", second_best_params_vp)
print("Best Victor-Purpura Distance:", best_distance_vp)
print("Second Best Victor-Purpura Distance:", second_best_distance_vp)

# Print the results for Van Rossum distance
print("Best Parameters for Van Rossum Distance:", best_params_vr2)
print("Second Best Parameters for Van Rossum Distance:", second_best_params_vr2)
print("Best Van Rossum Distance:", best_distance_vr2)
print("Second Best Van Rossum Distance:", second_best_distance_vr2)



gamma_values_plot = np.unique([entry['gamma'] for entry in vp_distances_list])
lambda_values_plot = np.unique([entry['lambda'] for entry in vp_distances_list])

# Create a 2D meshgrid of gamma and lambda values
gamma_mesh, lambda_mesh = np.meshgrid(gamma_values_plot, lambda_values_plot)

# Initialize an array to store total_distance values in the same grid shape
total_distance_mesh = np.zeros_like(gamma_mesh)

# Fill the total_distance values into the grid
for entry in vp_distances_list:
    gamma_idx = np.where(gamma_values_plot == entry['gamma'])[0][0]
    lambda_idx = np.where(lambda_values_plot == entry['lambda'])[0][0]
    total_distance_mesh[lambda_idx, gamma_idx] = entry['total_distance']

# Find the indices of the lowest total distance value
min_indices = np.unravel_index(np.argmin(total_distance_mesh), total_distance_mesh.shape)
min_gamma = gamma_values_plot[min_indices[1]]
min_lambda = lambda_values_plot[min_indices[0]]
min_distance = total_distance_mesh[min_indices]

# Create a contour plot
plt.figure(figsize=(10, 6))
contour = plt.contour(gamma_mesh, lambda_mesh, total_distance_mesh, levels=30, cmap='viridis')
plt.colorbar(contour, label='Total Distance')
plt.xlabel('Gamma')
plt.ylabel('Lambda')
plt.title('Contour Plot of Total Distance')
plt.scatter(min_gamma, min_lambda, color='red', s=100, label='Lowest Value')
plt.legend()

# Set x and y axis limits to range from 0 to 1
plt.xlim(0, 1)
plt.ylim(0, 1)

plt.grid()
plt.show()
